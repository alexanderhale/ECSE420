{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"FinalProject.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.4"}},"cells":[{"cell_type":"markdown","metadata":{"colab_type":"text","id":"uN68430x71-g"},"source":["# ECSE 420 - Final Project\n","Parallelized convolution, matrix inversion, and matrix multiplication algorithms."]},{"cell_type":"code","metadata":{"id":"jM5a276HAvqj","colab_type":"code","colab":{}},"source":["\n","\n"," !apt-get install nvidia-cuda-toolkit\n","!pip3 install numba\n","\n","import os\n","os.environ['NUMBAPRO_LIBDEVICE'] = \"/usr/lib/nvidia-cuda-toolkit/libdevice\"\n","os.environ['NUMBAPRO_NVVM'] = \"/usr/lib/x86_64-linux-gnu/libnvvm.so\""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Y7MQ6LteVQ3U","colab_type":"code","outputId":"8f6c959b-9605-4d96-8597-0941d1738b6a","executionInfo":{"status":"ok","timestamp":1574465221649,"user_tz":300,"elapsed":11240,"user":{"displayName":"Alex Hale","photoUrl":"","userId":"12012224760587127607"}},"colab":{"base_uri":"https://localhost:8080/","height":87}},"source":["import math\n","import multiprocessing\n","import random\n","import sys\n","import time\n","\n","\n","def merge(*args):\n","    # Support explicit left/right args, as well as a two-item tuple which works more cleanly with multiprocessing.\n","    left, right = args[0] if len(args) == 1 else args\n","    left_length, right_length = len(left), len(right)\n","    left_index, right_index = 0, 0\n","    merged = []\n","    while left_index < left_length and right_index < right_length:\n","        if left[left_index] <= right[right_index]:\n","            merged.append(left[left_index])\n","            left_index += 1\n","        else:\n","            merged.append(right[right_index])\n","            right_index += 1\n","    if left_index == left_length:\n","        merged.extend(right[right_index:])\n","    else:\n","        merged.extend(left[left_index:])\n","    return merged\n","\n","\n","def merge_sort(data):\n","    length = len(data)\n","    if length <= 1:\n","        return data\n","    middle = length // 2\n","    left = merge_sort(data[:middle])\n","    right = merge_sort(data[middle:])\n","    return merge(left, right)\n","\n","\n","def merge_sort_parallel(data):\n","    # Creates a pool of worker processes, one per CPU core.\n","    # We then split the initial data into partitions, sized\n","    # equally per worker, and perform a regular merge sort\n","    # across each partition.\n","    processes = multiprocessing.cpu_count()\n","    print(\"parallel processes: {}\".format(processes))\n","    pool = multiprocessing.Pool(processes=processes)\n","    size = int(math.ceil(float(len(data)) / processes))\n","    data = [data[i * size:(i + 1) * size] for i in range(processes)]\n","    data = pool.map(merge_sort, data)\n","    # Each partition is now sorted - we now just merge pairs of these\n","    # together using the worker pool, until the partitions are reduced\n","    # down to a single sorted result.\n","    while len(data) > 1:\n","        # If the number of partitions remaining is odd, we pop off the\n","        # last one and append it back after one iteration of this loop,\n","        # since we're only interested in pairs of partitions to merge.\n","        extra = data.pop() if len(data) % 2 == 1 else None\n","        data = [(data[i], data[i + 1]) for i in range(0, len(data), 2)]\n","        data = pool.map(merge, data) + ([extra] if extra else [])\n","    return data[0]\n","\n","\n","if __name__ == \"__main__\":\n","    results = []\n","    size = int(sys.argv[-1]) if sys.argv[-1].isdigit() else 1000000\n","    data_unsorted = [random.randint(0, size) for _ in range(size)]\n","    for sort in merge_sort, merge_sort_parallel:\n","        start = time.time()\n","        data_sorted = sort(data_unsorted)\n","        end = time.time() - start\n","        results.append(end)\n","        print(sort.__name__, end, sorted(data_unsorted) == data_sorted)\n","    print(\"speedup = {}\".format(results[0] / results[1]))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["merge_sort 5.2758166790008545 True\n","parallel processes: 4\n","merge_sort_parallel 3.1738831996917725 True\n","speedup = 1.6622592411444783\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"m00NZnuW_uJl","colab_type":"code","colab":{}},"source":["from numba import cuda\n","import numpy as np\n","import cv2\n","import matplotlib.pyplot as plt\n","import time\n","import random"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"kz4oFJWEsqOw","colab_type":"code","outputId":"ac639766-0a1f-4df3-a681-2e1b2e50a566","executionInfo":{"status":"ok","timestamp":1572917503938,"user_tz":300,"elapsed":322870,"user":{"displayName":"Alex Hale","photoUrl":"","userId":"12012224760587127607"}},"colab":{"base_uri":"https://localhost:8080/","height":208}},"source":["@cuda.jit\n","# perform full MergeSort on supplied subsection of data\n","def gpu_mergesort(source, dest, size, width, slices, numThreads, numBlocks):\n","  idx = 0 # TODO replace with proper ID of current thread (scheme from C++ ago)\n","  # int x;\n","  # idx = threadIdx.x +\n","  #         threadIdx.y * (x  = threads->x) +\n","  #         threadIdx.z * (x *= threads->y) +\n","  #         blockIdx.x  * (x *= threads->z) +\n","  #         blockIdx.y  * (x *= blocks->z) +\n","  #         blockIdx.z  * (x *= blocks->y);\n","  \n","  start = width * idx * slices\n","\n","  for s in slices:\n","    if start >= size:\n","      break\n","    \n","    middle = min(start + (width / 2), size)\n","    end = min(start + width, size)\n","    gpu_bottomUpMerge(source, dest, start, middle, end)\n","    start += width\n","\n","@cuda.jit\n","# perform full MergeSort on supplied subsection of data\n","def gpu_bottomUpMerge(source, dest, start, middle, end):\n","  i = start\n","  j = middle\n","\n","  for k in range(start, end):\n","    if i < middle and (j >= end || source[i] < source[j]):\n","      dest[j] = source[i]\n","      i += 1\n","    else:\n","      dest[k] = source[j]\n","      j += 1\n","\n","# number of threads and blocks used on the GPU (up to (1024, 1024, 1024))\n","threadsPerBlock = (256, 1, 1)\n","blocksPerGrid = (512, 1, 1)\n","totalThreads = threadsPerBlock[0] * threadsPerBlock[1] * threadsPerBlock[2]\n","                * blocksPerGrid[0] * blocksPerGrid[1] * blocksPerGrid[2]\n","\n","# create data array to be sorted\n","size = 100000\n","data = random.sample(range(size), size)\n","resultData = np.empty((data.shape))\n","\n","# give pieces of the list to each thread\n","width = 2   # start at smallest reasonable split size (2 elements)\n","start = time.time()\n","while (width < (size * 2)):\n","  slices = size / ((totalThreads) * width) + 1\n","  gpu_mergesort[blocksPerGrid, threadsPerBlock](data, resultData, size, width, slices, threadsPerBlock, blocksPerGrid)\n","\n","\n","  # tempData = data \n","  data = resultData\n","   \n","  # TODO do we need to \"switch the input/output arrays instead of copying them around?\"\n","    # data = data == D_data ? D_swp : D_data;\n","    # resultData = resultData == D_data ? D_swp : D_data;\n","  \n","  \n","\n","  width *= 2\n","\n","executionTime += time.time() - start\n","\n","# TODO print the execution time\n","# TODO verify that the array was sorted properly\n","# TODO compare execution time to recursive non-parallelized algorithm"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Average execution time is 0.0037560555934906007 with 1 threads.\n","Average execution time is 0.003610519886016846 with 2 threads.\n","Average execution time is 0.003678725481033325 with 4 threads.\n","Average execution time is 0.0037741470336914063 with 8 threads.\n","Average execution time is 0.004046255826950073 with 16 threads.\n","Average execution time is 0.004486570835113526 with 32 threads.\n","Average execution time is 0.0044991190433502195 with 64 threads.\n","Average execution time is 0.00448096489906311 with 128 threads.\n","Average execution time is 0.004564995765686035 with 256 threads.\n","Average execution time is 0.005304852247238159 with 512 threads.\n","Average execution time is 0.00730977201461792 with 1024 threads.\n"],"name":"stdout"}]}]}